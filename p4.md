# TO-DOs
- [x] Use multi-list queue in scheduler.
- [x] Implement CPU-bound criterion.
- [x] Implement CPU rescheduling request.
- [x] Implement CPU-auto-distribution criterion.

# Tópicos
- [x] Execução do escalonador multi-head round-robin.
  - [x] Mostrar como a execução é distribuida entre os processadores.
- [x] Execução do CPU-bound round-robin.  
  - [x] Mostrar como a execução é feita somente na CPU zero.
- [x] Execução com distribuição manual.
  - [x] Mostrar como executa os três primeiros filósofos na três primeiras CPUs.
  - [x] Também mostrar como acelera a execução.
    - [x] Provavelmente por não haver cache misses.
- [ ] Execução com o tempo de iniciação (S)  
    - [ ] Mostrar que às vezes há uma demora na iniciação dos threads.
- [ ] Executar com ipi_send
    - [ ] Mostrar como adianta a iniciação dos threads.
- [ ] Executar com escalonador e ipi_send desligado
    - [ ] Mostrar como a execução do thread 4 só começa depois do thread 5.
- [ ] Executar com escalonador desligado e ipi_send ligado.
    - [ ] Mostrar como thread 4 como a executar logo em seguida.
- [ ] Executar distribuição automatica de threads para as CPUs.

# INE5424 - SO II - P4: CPU Affinity Scheduling

Alunos:
- Glaucia de Pádua da Silva - 09232087
- Quenio Cesar Machado dos Santos - 14100868

## Verificando Tempo de Execução em CPUs

Para melhor entender como se dá a alocação das CPUs para cada política de escalonamento, modificamos a classe `Thread` para capturar de tempo total de execução dos `threads` em cada CPU. Também modificamos o programa "jantar dos filósofos" para imprimir os tempos ao encerrar sua execução.

As mudanças na classe `Thread` estão a seguir:

```cpp
class Thread
{
   ...
   typedef Scheduler_Timer Timer;

public:
   ...
   int total_tick(int cpu_id) { return _total_tick_[cpu_id]; }

private:
   ...
   Timer::Tick _tick_count_, _total_tick_[Traits<Build>::CPUS];
   ...
}
...
void Thread::dispatch(Thread * prev, Thread * next, bool charge)
{
    if(charge) {
        if(Criterion::timed)
            _timer_->reset();
    }

    if(prev != next) {
        next->_tick_count_ = Timer::tick_count();
        if (prev->_state_ == RUNNING || prev->_state_ == FINISHING) {
          prev->_total_tick_[Machine::cpu_id()] += (next->_tick_count_ - prev->_tick_count_);
        }

        if(prev->_state_ == RUNNING) {
            prev->_state_ = READY;
        }
        next->_state_ = RUNNING;

        spinUnlock();

        CPU::switch_context(&prev->_context_, next->_context_);
    } else {
      spinUnlock();
    }

    CPU::int_enable();
}
```

No código acima, observe que o método `tick_count(cpu_id)` irá retornar o tempo em millisegundos que um `Thread` executou numa determinada CPU. O monitoramento e cálculo do tempo é feito no método `dispatch()`.

O programa do "jantar dos filósofos" vai utilizar o método `tick_count()` para apresentar os tempos de todos os `threads` - um por "filósofo" - ao final de sua execução. Veja abaixo:

```cpp
int main()
{
  ...
  cout << "The dinner is served ..." << endl;
  table.unlock();

  Timer::Tick total_cpu_tick[Traits<Build>::CPUS];
  for(int i = 0; i < 5; i++) {
      int ret = phil[i]->join();
      table.lock();
      Display::position(20 + i, 0);
      cout << "Philosopher " << i;
      Timer::Tick total_thread_tick = 0;
      for (int cpu_id = 0; cpu_id < Machine::n_cpus(); cpu_id++) {
        Timer::Tick tick_per_cpu = phil[i]->total_tick(cpu_id);
        total_thread_tick += tick_per_cpu;
        total_cpu_tick[cpu_id] += tick_per_cpu;
        cout << " | " << cpu_id << ": " << Spaced(tick_per_cpu);
      }
      cout << " | T: " << Spaced(total_thread_tick) << endl;
      table.unlock();
  }
  table.lock();
  Display::position(25, 0);
  cout << "CPU Totals   ";
  Timer::Tick total_tick = 0;
  for (int cpu_id = 0; cpu_id < Machine::n_cpus(); cpu_id++) {
    Timer::Tick tick_per_cpu = total_cpu_tick[cpu_id];
    total_tick += tick_per_cpu;
    cout << " | " << cpu_id << ": " << Spaced(tick_per_cpu);
  }
  cout << " | T: " << Spaced(total_tick) << endl << endl;
  table.unlock();
  ...
}
```

## A Alocação de CPU do Multi-Head Round-Robin

No trabalho P3, nós implementamos o round-robin usando uma fila `multi-head` para que as CPUs pudessem compartilhar a fila de `threads`. Isto fez com que os `threads` fossem executados em todas as CPUs, como se vê na saída abaixo:

```
qemu-system-i386 -smp 4 -m 262144k -nographic -no-reboot -fda  phil_dinner.img | tee phil_dinner.out
Setting up this machine as follows:
  Processor:    IA32 at 4002 MHz (BUS clock = 125 MHz)
  Memory:       262144 Kbytes [0x00000000:0x10000000]
  User memory:  261752 Kbytes [0x00000000:0x0ff9e000]
  PCI aperture: 44996 Kbytes [0xfc000000:0xfebf1000]
  Node Id:      will get from the network!
  Setup:        21344 bytes
  APP code:     32608 bytes	data: 640 bytes
  CPU count:    4

The Philosopher's Dinner:
Philosophers are alive and hungry!
even numbers: 15000-3

                                 done   2

                          \                /


                     done   3                done   3


                          /                \


                         done   0 |     done   1


The dinner is served ...
Philosopher 0 | 0:  1235 | 1:   190 | 2:  4096 | 3:   110 | T:  5631
Philosopher 1 | 0:  1364 | 1:   138 | 2:  3862 | 3:   241 | T:  5605
Philosopher 2 | 0:  3359 | 1:   110 | 2:  2598 | 3:   110 | T:  6177
Philosopher 3 | 0:  1495 | 1:   107 | 2:  1292 | 3:  1480 | T:  4374
Philosopher 4 | 0:  1200 | 1:   290 | 2:  1358 | 3:  2028 | T:  4876
CPU Totals    | 0:  8653 | 1:   835 | 2: 13206 | 3:  3969 | T: 26663
```

Observamos o seguinte nesta saída:
- Cada um dos `threads` foi executado em todas as quatro CPUs disponíveis.
- O tempo total de execução de cada `thread` variou bastante.
- O tempo de uso de cada CPU também variou.
- Tempo total de execução em todas as CPUs foi 26s.

## CPU-Bound

O problema com a execução acima é que, ao migrar a execução de cada `thread` entre as CPUs, o escalonador está gerando uma série de `cache misses`. Os `caches` da CPUs são invalidados toda vez que o `working set` de um `thread` precisa migrar para outra CPU. Isto causa queda no desempenho de execução.

Para sanar este problema, implementamos uma versão do round-robin que é `CPU-bound`:

```cpp
...
template<typename T, typename R = typename T::Criterion>
class Scheduling_Queue: public Scheduling_Multilist<T> {};
...
class CPU_Bound
{
public:
  static const unsigned int QUEUES = Traits<Machine>::CPUS;

  static unsigned int current_queue() { return Machine::cpu_id(); }

public:
  CPU_Bound(unsigned int queue = current_queue()): _queue_(queue) {}

  const volatile unsigned int & queue() const volatile { return _queue_; }

private:
  volatile unsigned int _queue_;
};
...
class CPU_Bound_RR: public RR, public CPU_Bound
{
public:
  CPU_Bound_RR(int p = NORMAL, unsigned int queue = current_queue())
    : RR(p), CPU_Bound(queue) {}
};
...
template<> struct Traits<Thread>: public Traits<void>
{
  ...
  typedef Scheduling_Criteria::CPU_Bound_RR Criterion;
  ...
};
...
```  

Observe acima que `Scheduling_Queue` agora é uma multi-lista. Isto permite que cada CPU tenha sua própria fila. Quando for entregar o próximo `thread` a `Scheduler`, a classe `Scheduling_Multilist` vai usar o método `current_queue()` de `CPU_Bound` para escolher a fila da CPU corrente . Além disso, observe também que o método `queue()` diz em que fila (ou CPU) o `Thread` se encontra. Por `default`, `CPU_Bound_RR` vai colocar um `thread` na mesma CPU do `thread` que o criou.

A saída abaixo mostra o compartamento de `CPU_Bound_RR`:

```
qemu-system-i386 -smp 4 -m 262144k -nographic -no-reboot -fda  phil_dinner.img | tee phil_dinner.out
Setting up this machine as follows:
  Processor:    IA32 at 452 MHz (BUS clock = 125 MHz)
  Memory:       262144 Kbytes [0x00000000:0x10000000]
  User memory:  261752 Kbytes [0x00000000:0x0ff9e000]
  PCI aperture: 44996 Kbytes [0xfc000000:0xfebf1000]
  Node Id:      will get from the network!
  Setup:        21344 bytes
  APP code:     32976 bytes	data: 704 bytes
  CPU count:    4

The Philosopher's Dinner:
Philosophers are alive and hungry!
even numbers: 15000-0

                                 done   0

                          \                /


                     done   0                done   0


                          /                \


                         done   0 |     done   0


The dinner is served ...
Philosopher 0 | 0:  5488 | 1:     0 | 2:     0 | 3:     0 | T:  5488
Philosopher 1 | 0:  5353 | 1:     0 | 2:     0 | 3:     0 | T:  5353
Philosopher 2 | 0:  5175 | 1:     0 | 2:     0 | 3:     0 | T:  5175
Philosopher 3 | 0:  5642 | 1:     0 | 2:     0 | 3:     0 | T:  5642
Philosopher 4 | 0:  5094 | 1:     0 | 2:     0 | 3:     0 | T:  5094
CPU Totals    | 0: 26752 | 1:     0 | 2:     0 | 3:     0 | T: 26752

The end!
The last thread has exited!
Rebooting the machine ...
```

Observe na saída acima que todos os threads acabaram executando somente na CPU zero. Isto se deu porque foi a partir da CPU zero que os demais `threads` foram criados, de acordo com o compartamento `default` de `CPU_Bound_RR`.

No entanto, é possível alterar em qual CPU um `thread` será executado:

```cpp
int main()
{
    table.lock();
    Display::clear();
    Display::position(0, 0);
    cout << "The Philosopher's Dinner:" << endl;

    for(int i = 0; i < 5; i++)
        chopstick[i] = new Semaphore;

    phil[0] = new Thread(
      Thread::Configuration(Thread::READY, Thread::Criterion(Thread::NORMAL, 0)), // CPU 0
      &philosopher, 0,  5, 32);
    phil[1] = new Thread(
      Thread::Configuration(Thread::READY, Thread::Criterion(Thread::NORMAL, 1)), // CPU 1
      &philosopher, 1, 10, 44);
    phil[2] = new Thread(
      Thread::Configuration(Thread::READY, Thread::Criterion(Thread::NORMAL, 2)), // CPU 2
      &philosopher, 2, 16, 39);
    phil[3] = new Thread(
      Thread::Configuration(Thread::READY, Thread::Criterion(Thread::NORMAL, 3)), // CPU 3
      &philosopher, 3, 16, 24);
    phil[4] = new Thread(
      Thread::Configuration(Thread::READY, Thread::Criterion(Thread::NORMAL, 3)), // CPU 3 - novamente
      &philosopher, 4, 10, 20);

    cout << "Philosophers are alive and hungry!" << endl;
    ...
}
```

Observe que a seguinte alocação para os `threads` do "filósofos":
- `thread` zero na CPU zero;
- `thread` um na CPU um;
- 'thread' dois na CPU dois
- 'threads' três e quatro na CPU três.

Ao executar o "jantar dos filósofos" após estas alterações:

```
qemu-system-i386 -smp 4 -m 262144k -nographic -no-reboot -fda  phil_dinner.img | tee phil_dinner.out
Setting up this machine as follows:
  Processor:    IA32 at 422 MHz (BUS clock = 125 MHz)
  Memory:       262144 Kbytes [0x00000000:0x10000000]
  User memory:  261752 Kbytes [0x00000000:0x0ff9e000]
  PCI aperture: 44996 Kbytes [0xfc000000:0xfebf1000]
  Node Id:      will get from the network!
  Setup:        21344 bytes
  APP code:     33296 bytes	data: 704 bytes
  CPU count:    4

The Philosopher's Dinner:
Philosophers are alive and hungry!
even numbers: 15000-3

                                 done   0

                          \                /


                     done   3                done   1


                          /                \


                         done   3 |     done   2


The dinner is served ...
Philosopher 0 | 0:  1166 | 1:     0 | 2:     0 | 3:     0 | T:  1166
Philosopher 1 | 0:     0 | 1:  1071 | 2:     0 | 3:     0 | T:  1071
Philosopher 2 | 0:     0 | 1:     0 | 2:  1108 | 3:     0 | T:  1108
Philosopher 3 | 0:     0 | 1:     0 | 2:     0 | 3:  6797 | T:  6797
Philosopher 4 | 0:     0 | 1:     0 | 2:     0 | 3:  6421 | T:  6421
CPU Totals    | 0:  1166 | 1:  1071 | 2:  1108 | 3: 13218 | T: 16563

The end!
The last thread has exited!
Rebooting the machine ...
```

Na saída acima, vale observar que:
- A alocação de CPUs para cada `thread` foi respeitada pelo escalonador.
- O tempo total de execução dos `threads` 0, 1 e 2 diminuiu bastante quando comparado à execução anterior. Isto ocorreu provavelmente porque não houve troca de contexto e `cache misses`.
- Entretanto, os `threads` 3 e 4, que compartilharam a mesa CPU, continuaram levando mais tempo. Provavelmente pelas trocas de contexto e 'cache invalidation'.
- O tempo total de todas as CPUs diminuiu de 26s para 16.5s.
